{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Crawl pages whose seed url is the press releases page of the Securities and Exchange Commission: https://www.sec.gov/news/pressreleases and collect urls of press releases that contain the word \"charges\". The code should output the first 20 such links that it finds. For each link output the url and the text. Similar to the previous task, when checking for the presence of the word \"charges\" you should consider lower- and upper-case versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with url=['https://www.sec.gov/news/pressreleases']\n",
      "num. of URLs in stack: 0 \n",
      "Trying to access= https://www.sec.gov/news/pressreleases\n",
      "# 1: https://www.sec.gov/news/sec-webcasts\n",
      "\n",
      "# 2: https://www.sec.gov/news/press-release/2022-183\n",
      "\n",
      "# 3: https://www.sec.gov/news/press-release/2022-182\n",
      "\n",
      "# 4: https://www.sec.gov/news/press-release/2022-181\n",
      "\n",
      "# 5: https://www.sec.gov/news/press-release/2022-180\n",
      "\n",
      "# 6: https://www.sec.gov/news/press-release/2022-179\n",
      "\n",
      "# 7: https://www.sec.gov/news/press-release/2022-178\n",
      "\n",
      "# 8: https://www.sec.gov/news/press-release/2022-176\n",
      "\n",
      "# 9: https://www.sec.gov/news/press-release/2022-175\n",
      "\n",
      "# 10: https://www.sec.gov/news/press-release/2022-174\n",
      "\n",
      "# 11: https://www.sec.gov/news/press-release/2022-173\n",
      "\n",
      "# 12: https://www.sec.gov/news/press-release/2022-172\n",
      "\n",
      "# 13: https://www.sec.gov/news/press-release/2022-171\n",
      "\n",
      "# 14: https://www.sec.gov/news/press-release/2022-170\n",
      "\n",
      "# 15: https://www.sec.gov/news/press-release/2022-169\n",
      "\n",
      "# 16: https://www.sec.gov/news/press-release/2022-168\n",
      "\n",
      "# 17: https://www.sec.gov/news/press-release/2022-167\n",
      "\n",
      "# 18: https://www.sec.gov/news/press-release/2022-164\n",
      "\n",
      "# 19: https://www.sec.gov/news/press-release/2022-163\n",
      "\n",
      "# 20: https://www.sec.gov/news/press-release/2022-161\n",
      "\n",
      "num. of URLs seen = 41, and scanned = 1\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Crawl 10 Pages with word \"covid\" and store in List\n",
    "seed_url = \"https://www.sec.gov/news/pressreleases\"\n",
    "domain = \"https://www.sec.gov/news\"\n",
    "\n",
    "urls = [seed_url]  # queue of urls to crawl\n",
    "seen = [seed_url]  # stack of urls seen so far\n",
    "opened = []\n",
    "charges_urls = []\n",
    "\n",
    "maxNumUrl = 20;  # set the maximum number of urls included 'covid' to visit\n",
    "print(\"Starting with url=\" + str(urls))\n",
    "while len(urls) > 0 and len(charges_urls) < maxNumUrl:\n",
    "    # DEQUEUE A URL FROM urls AND TRY TO OPEN AND READ IT\n",
    "    try:\n",
    "        curr_url = urls.pop(0)  # popï¼šdelete the first item of the list\n",
    "        print(\"num. of URLs in stack: %d \" % len(urls))\n",
    "        print(\"Trying to access= \" + curr_url)\n",
    "        req = urllib.request.Request(curr_url, headers={'User-Agent': 'Mozilla/5.0'}) \n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        # print(webpage)\n",
    "        opened.append(curr_url)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Unable to access= \" + curr_url)\n",
    "        print(ex)\n",
    "        continue  # skip code below\n",
    "\n",
    "    # IF URL OPENS, CHECK WHICH URLS THE PAGE CONTAINS\n",
    "    # ADD THE URLS FOUND TO THE QUEUE url AND seen\n",
    "    soup = BeautifulSoup(webpage)  # creates object soup \n",
    "\n",
    "    for tag in soup.find_all('a', href=True):  # find tags with links\n",
    "        childUrl = tag['href']  # extract just the link\n",
    "        childUrl = urllib.parse.urljoin(seed_url, childUrl)  # join two urls\n",
    "        if domain in childUrl and childUrl not in seen:\n",
    "            urls.append(childUrl)\n",
    "            seen.append(childUrl)\n",
    "\n",
    "            # search 'charges' in the press release text\n",
    "            response = requests.get(childUrl)\n",
    "            soup_child = BeautifulSoup(response.text, 'html.parser')\n",
    "            keyword_found = 0\n",
    "            # gather title, release number, location, publish date, and article body.\n",
    "            article = soup_child.find_all('h1', class_='article-title')+soup_child.find_all('p', class_='release-number')+soup_child.find_all('p', class_='article-location-publishdate')+soup_child.find_all('div', class_='article-body')\n",
    "            if len(article): # this webpage contains an article (press release)\n",
    "                for item in article:\n",
    "                    if 'charges' in item.text.lower():\n",
    "                        keyword_found = 1\n",
    "                        break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if keyword_found:\n",
    "                charges_urls.append(childUrl)\n",
    "                print('# '+str(len(charges_urls))+': '+ childUrl +\"\\n\")\n",
    "\n",
    "                text_output_path = './press_releases_that_contain_CHARGES/'+str(len(charges_urls))+'.txt'\n",
    "                \n",
    "                # put the URL at the beginning of the output file\n",
    "                output_text = [childUrl+'\\n',] \n",
    "                \n",
    "                for item in article:\n",
    "                    output_text.append(item.text)\n",
    "                # print the press releases that contain 'charges' to seperate .txt files\n",
    "                with open(text_output_path, \"w+\", encoding='utf-8') as foutput: \n",
    "                    foutput.writelines(output_text)\n",
    "\n",
    "                if len(charges_urls) >= maxNumUrl:\n",
    "                    break\n",
    "\n",
    "print(\"num. of URLs seen = %d, and scanned = %d\" % (len(seen), len(opened)))\n",
    "\n",
    "# Output 20 links with word \"covid\" in csv file\n",
    "import pandas as pd\n",
    "charges_links = pd.DataFrame(charges_urls)\n",
    "charges_links.columns = ['charges_links']\n",
    "charges_links.to_csv('charges_links.csv',mode='w+',header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
