{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 (80 points)\n",
    "\n",
    "In this question you are tasked with writing modified versions of the web crawler that we covered in class. More specifically, you asked to create two webcrawlers for the following tasks:\n",
    "\n",
    "1.Crawl pages whose seed url is the press releases page of the Federal Reserve System: https://www.federalreserve.gov/newsevents/pressreleases.htm and collect pages that contain the word \"covid\" found within the page. The goal is to collect at least 10 such urls. At the end of the crawling the code should output the urls of the webpages found to contain the word \"covid\". When checking whether the word is present on the webpage you should consider lower- and upper-case word versions (Covid, COVID, covid). One way to do this is to lowercase the webpage text prior to doing word matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with url=['https://www.federalreserve.gov/newsevents/pressreleases.htm']\n",
      "num. of URLs in stack: 0 \n",
      "Trying to access= https://www.federalreserve.gov/newsevents/pressreleases.htm\n",
      "num. of URLs in stack: 21 \n",
      "Trying to access= https://www.federalreserve.gov/newsevents/pressreleases.htm#content\n",
      "num. of URLs in stack: 20 \n",
      "Trying to access= https://www.federalreserve.gov/newsevents/pressreleases/2022-press-fomc.htm\n",
      "num. of URLs in stack: 32 \n",
      "Trying to access= https://www.federalreserve.gov/newsevents/pressreleases/2022-press.htm\n",
      "num. of URLs seen = 147, and scanned = 4\n"
     ]
    }
   ],
   "source": [
    "# Crawl 10 Pages with word \"covid\" and store in List\n",
    "seed_url = \"https://www.federalreserve.gov/newsevents/pressreleases.htm\"\n",
    "domain = \"https://www.federalreserve.gov/newsevents/pressreleases\"\n",
    "\n",
    "urls = [seed_url]  # queue of urls to crawl\n",
    "seen = [seed_url]  # stack of urls seen so far\n",
    "opened = []\n",
    "covidurl = []\n",
    "\n",
    "maxNumUrl = 10;  # set the maximum number of urls included 'covid' to visit\n",
    "print(\"Starting with url=\" + str(urls))\n",
    "while len(urls) > 0 and len(covidurl) < maxNumUrl:\n",
    "    # DEQUEUE A URL FROM urls AND TRY TO OPEN AND READ IT\n",
    "    try:\n",
    "        curr_url = urls.pop(0)  # popï¼šdelete the first item of the list\n",
    "        print(\"num. of URLs in stack: %d \" % len(urls))\n",
    "        print(\"Trying to access= \" + curr_url)\n",
    "        req = urllib.request.Request(curr_url, headers={'User-Agent': 'Mozilla/5.0'}) \n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        opened.append(curr_url)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Unable to access= \" + curr_url)\n",
    "        print(ex)\n",
    "        continue  # skip code below\n",
    "\n",
    "    # IF URL OPENS, CHECK WHICH URLS THE PAGE CONTAINS\n",
    "    # ADD THE URLS FOUND TO THE QUEUE url AND seen\n",
    "    soup = BeautifulSoup(webpage)  # creates object soup \n",
    "    \n",
    "    # Put child URLs into the stack\n",
    "    for tag in soup.find_all('a', href=True):  # find tags with links\n",
    "        childUrl = tag['href']  # extract just the link\n",
    "        childUrl = urllib.parse.urljoin(seed_url, childUrl)  # join two urls\n",
    "        if domain in childUrl and childUrl not in seen:\n",
    "            urls.append(childUrl)\n",
    "            seen.append(childUrl)\n",
    "            if 'covid' in requests.get(childUrl).text.lower():  #find childurls with \"covid\"\n",
    "                covidurl.append(childUrl)\n",
    "                if len(covidurl) >= maxNumUrl:\n",
    "                    break\n",
    "\n",
    "print(\"num. of URLs seen = %d, and scanned = %d\" % (len(seen), len(opened)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 10 links with word \"covid\" in csv file\n",
    "import pandas as pd\n",
    "covid_links = pd.DataFrame(covidurl)\n",
    "covid_links.columns = ['covid_links']\n",
    "covid_links.to_csv('covid_links.csv',mode='w+',header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
